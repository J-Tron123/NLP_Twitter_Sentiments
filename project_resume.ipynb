{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, warnings\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from models import CleanData\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv(\"data/tweets_pos_clean.csv\")\n",
    "negative = pd.read_csv(\"data/tweets_neg_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive[\"Target\"] = [0 for i in positive[\"Tweets\"]]\n",
    "negative[\"Target\"] = [1 for i in negative[\"Tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives Tweets: 55056\n",
      "Negative Tweets: 120948\n"
     ]
    }
   ],
   "source": [
    "print(\"Positives Tweets:\", len(positive))\n",
    "print(\"Negative Tweets:\", len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Se imaginan a los chicos agradeciendo por el p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclesiastes4:9-12 ‚ô° Siempre, promesa :)  https...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pedroj_ramirez Qu√© sabor√≠o, PJ. ya no compart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buenos dias para todos. Feliz inicio de semana...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@pepedom @bquintero Gracias! No es as√≠, deja c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175999</th>\n",
       "      <td>Pero... Dime que no te perder√© del todo :( ‚ù§üíõüíö</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176000</th>\n",
       "      <td>Yo creo que a Colocolo le hac√≠a falta un parti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176001</th>\n",
       "      <td>@seru15 son para ni√±o :( quisiera quedarmelos.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176002</th>\n",
       "      <td>Diganle al sonidero que ya le baje a su desmad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176003</th>\n",
       "      <td>Buenos d√≠as, gente que se le da√±a el celular :(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176004 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweets  Target\n",
       "0       Se imaginan a los chicos agradeciendo por el p...       0\n",
       "1       Eclesiastes4:9-12 ‚ô° Siempre, promesa :)  https...       0\n",
       "2       @pedroj_ramirez Qu√© sabor√≠o, PJ. ya no compart...       0\n",
       "3       Buenos dias para todos. Feliz inicio de semana...       0\n",
       "4       @pepedom @bquintero Gracias! No es as√≠, deja c...       0\n",
       "...                                                   ...     ...\n",
       "175999     Pero... Dime que no te perder√© del todo :( ‚ù§üíõüíö       1\n",
       "176000  Yo creo que a Colocolo le hac√≠a falta un parti...       1\n",
       "176001     @seru15 son para ni√±o :( quisiera quedarmelos.       1\n",
       "176002  Diganle al sonidero que ya le baje a su desmad...       1\n",
       "176003    Buenos d√≠as, gente que se le da√±a el celular :(       1\n",
       "\n",
       "[176004 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = positive.merge(negative, how=\"outer\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_links)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().clean_emojis)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_stopwords)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().signs_tweets)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_doubles)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().clean_laughs)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_mentions_hashtags)\n",
    "\n",
    "# df.to_csv(\"data/data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_cleaned.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se imaginan chicos agradeciendo premio cara or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eclesiastes siempre promesa  {link}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pedroj_ramirez qu√© sabor√≠o pj compartes ginton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buenos dias todos feliz inicio semana  {link}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pepedom bquintero gracias no as√≠ deja claro  a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175999</th>\n",
       "      <td>pero dime perder√©</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176000</th>\n",
       "      <td>yo creo colocolo hac√≠a falta partido as√≠ mas p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176001</th>\n",
       "      <td>seru ni√±o  quisiera quedarmelos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176002</th>\n",
       "      <td>diganle sonidero baje desmadre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176003</th>\n",
       "      <td>buenos d√≠as gente da√±a celular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175975 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweets\n",
       "0       se imaginan chicos agradeciendo premio cara or...\n",
       "1                     eclesiastes siempre promesa  {link}\n",
       "2       pedroj_ramirez qu√© sabor√≠o pj compartes ginton...\n",
       "3           buenos dias todos feliz inicio semana  {link}\n",
       "4       pepedom bquintero gracias no as√≠ deja claro  a...\n",
       "...                                                   ...\n",
       "175999                                 pero dime perder√© \n",
       "176000  yo creo colocolo hac√≠a falta partido as√≠ mas p...\n",
       "176001                    seru ni√±o  quisiera quedarmelos\n",
       "176002                    diganle sonidero baje desmadre \n",
       "176003                    buenos d√≠as gente da√±a celular \n",
       "\n",
       "[175975 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jbartolomero c√≥mo ves econom√≠a asi√°tica hasta punto crees q afectar√° a da miedoo  buff'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Tweets[170001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################################\n",
    "\n",
    "logistic_pipe = Pipeline([(\"vect\", vectorizer), (\"cls\", LogisticRegression())]) # Logistic Regression\n",
    "\n",
    "logistic_params = {\"vect__max_df\": (0.5, 1), \"vect__min_df\": (10, 20, 50), \"cls__penalty\": [\"l1\",\"l2\"], \n",
    "\"cls__C\": [0.1, 1.0], \"cls__solver\" : [\"newton-cg\"]}\n",
    "\n",
    "log_reg = GridSearchCV(logistic_pipe, logistic_params, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "svc_pipe = Pipeline([(\"vect\", vectorizer), (\"cls\", LinearSVC())]) # Linear Support Vector Machine\n",
    "\n",
    "svc_params = {\"cls__C\": [0.001, 0.1, 1, 10, 100], \"cls__loss\": [\"hinge\", \"squared_hinge\"], \"cls__penalty\" : [\"l1\", \"l2\"]}\n",
    "\n",
    "svc = GridSearchCV(svc_pipe, svc_params, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "xgb = Pipeline([(\"vect\", vectorizer), (\"cls\", XGBClassifier())]) # XGB Classifier\n",
    "\n",
    "#####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Tweets\"], df[\"Target\"], test_size=0.20, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       ('cls', LogisticRegression())]),\n",
       "             param_grid={'cls__C': [0.1, 1.0], 'cls__penalty': ['l1', 'l2'],\n",
       "                         'cls__solver': ['newton-cg'], 'vect__max_df': (0.5, 1),\n",
       "                         'vect__min_df': (10, 20, 50)},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112231851115215\n"
     ]
    }
   ],
   "source": [
    "log_reg_predictions = log_reg.predict(X_test)\n",
    "\n",
    "log_reg_accuraccy = accuracy_score(log_reg_predictions, y_test)\n",
    "\n",
    "print(log_reg_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       ('cls', LinearSVC())]),\n",
       "             param_grid={'cls__C': [0.001, 0.1, 1, 10, 100],\n",
       "                         'cls__loss': ['hinge', 'squared_hinge'],\n",
       "                         'cls__penalty': ['l1', 'l2']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8177013780366529\n"
     ]
    }
   ],
   "source": [
    "svc_predictions = svc.predict(X_test)\n",
    "\n",
    "svc_accuraccy = accuracy_score(svc_predictions, y_test)\n",
    "\n",
    "print(svc_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('cls',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints='()',\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               predictor='auto', random_state=0, reg_alpha=0,\n",
       "                               reg_lambda=1, ...))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791589714448075\n"
     ]
    }
   ],
   "source": [
    "xgb_predictions = xgb.predict(X_test)\n",
    "\n",
    "xgb_accuraccy = accuracy_score(xgb_predictions, y_test)\n",
    "\n",
    "print(xgb_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the strings into integers using Tokenizer\n",
    "\n",
    "max_vocab = 20000000\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of dataset vocab is: 128416\n"
     ]
    }
   ],
   "source": [
    "# Checking the word index and find out the vocabulary of the dataset\n",
    "\n",
    "wordidx = tokenizer.word_index\n",
    "\n",
    "print(f\"The size of dataset vocab is: {len(wordidx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequence: [50, 67, 72, 36, 16, 1]\n",
      "Test sequence: [675, 21]\n"
     ]
    }
   ],
   "source": [
    "# Converting train and test sentences into sequences\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "print(f\"Train sequence: {train_seq[0]}\")\n",
    "print(f\"Test sequence: {test_seq[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The len of train sequence is: 2095\n",
      "The len of test sequence is: 2095\n"
     ]
    }
   ],
   "source": [
    "# Padding the sentences to get equal length sequence because it's conventional to use same size sequences\n",
    "\n",
    "# Padding Train\n",
    "pad_train = pad_sequences(train_seq)\n",
    "\n",
    "print(f\"The len of train sequence is: {pad_train.shape[1]}\")\n",
    "\n",
    "\n",
    "# Padding test\n",
    "pad_test = pad_sequences(test_seq, maxlen=pad_train.shape[1])\n",
    "\n",
    "print(f\"The len of test sequence is: {pad_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "\n",
    "input_len = keras.layers.Input(shape=(pad_train.shape[1], ))\n",
    "\n",
    "x = keras.layers.Embedding(len(wordidx) + 1, 20)(input_len) # len(wordidx) + 1 because the indexing starts from 1, not from 0\n",
    "\n",
    "x = keras.layers.LSTM(25, return_sequences=True)(x)\n",
    "\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "neural_network_model = keras.Model(input_len, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "neural_network_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "mcheckpoint = keras.callbacks.ModelCheckpoint(\"data/models/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4400/4400 [==============================] - 388s 88ms/step - loss: 0.4428 - accuracy: 0.8000 - val_loss: 0.4104 - val_accuracy: 0.8166\n",
      "Epoch 2/10\n",
      "4400/4400 [==============================] - 424s 96ms/step - loss: 0.3021 - accuracy: 0.8765 - val_loss: 0.4549 - val_accuracy: 0.8086\n",
      "Epoch 3/10\n",
      "4400/4400 [==============================] - 419s 95ms/step - loss: 0.2086 - accuracy: 0.9189 - val_loss: 0.4851 - val_accuracy: 0.8010\n",
      "Epoch 4/10\n",
      "4400/4400 [==============================] - 421s 96ms/step - loss: 0.1543 - accuracy: 0.9408 - val_loss: 0.5688 - val_accuracy: 0.7910\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "history = neural_network_model.fit(pad_train, y_train, validation_data=(pad_test, y_test), epochs=10, callbacks=[earlystop, mcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 2095)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 2095, 20)          2568340   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 2095, 25)          4600      \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 25)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                832       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,573,805\n",
      "Trainable params: 2,573,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_network_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 33s 30ms/step - loss: 0.4104 - accuracy: 0.8166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41043326258659363, 0.8165932893753052]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_model.evaluate(pad_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bdc6e612fb2213545a98f8a0a443d810ee75e809c56c9dac704b481d62b9359"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
