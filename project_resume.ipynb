{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, keras, pickle, warnings\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from models import CleanData\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv(\"data/tweets_pos_clean.csv\")\n",
    "negative = pd.read_csv(\"data/tweets_neg_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive[\"Target\"] = [0 for i in positive[\"Tweets\"]]\n",
    "negative[\"Target\"] = [1 for i in negative[\"Tweets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives Tweets: 55056\n",
      "Negative Tweets: 120948\n"
     ]
    }
   ],
   "source": [
    "print(\"Positives Tweets:\", len(positive))\n",
    "print(\"Negative Tweets:\", len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Se imaginan a los chicos agradeciendo por el p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclesiastes4:9-12 ‚ô° Siempre, promesa :)  https...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pedroj_ramirez Qu√© sabor√≠o, PJ. ya no compart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buenos dias para todos. Feliz inicio de semana...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@pepedom @bquintero Gracias! No es as√≠, deja c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175999</th>\n",
       "      <td>Pero... Dime que no te perder√© del todo :( ‚ù§üíõüíö</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176000</th>\n",
       "      <td>Yo creo que a Colocolo le hac√≠a falta un parti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176001</th>\n",
       "      <td>@seru15 son para ni√±o :( quisiera quedarmelos.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176002</th>\n",
       "      <td>Diganle al sonidero que ya le baje a su desmad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176003</th>\n",
       "      <td>Buenos d√≠as, gente que se le da√±a el celular :(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176004 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweets  Target\n",
       "0       Se imaginan a los chicos agradeciendo por el p...       0\n",
       "1       Eclesiastes4:9-12 ‚ô° Siempre, promesa :)  https...       0\n",
       "2       @pedroj_ramirez Qu√© sabor√≠o, PJ. ya no compart...       0\n",
       "3       Buenos dias para todos. Feliz inicio de semana...       0\n",
       "4       @pepedom @bquintero Gracias! No es as√≠, deja c...       0\n",
       "...                                                   ...     ...\n",
       "175999     Pero... Dime que no te perder√© del todo :( ‚ù§üíõüíö       1\n",
       "176000  Yo creo que a Colocolo le hac√≠a falta un parti...       1\n",
       "176001     @seru15 son para ni√±o :( quisiera quedarmelos.       1\n",
       "176002  Diganle al sonidero que ya le baje a su desmad...       1\n",
       "176003    Buenos d√≠as, gente que se le da√±a el celular :(       1\n",
       "\n",
       "[176004 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = positive.merge(negative, how=\"outer\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_links)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().clean_emojis)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_stopwords)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().signs_tweets)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_doubles)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().clean_laughs)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_mentions_hashtags_retweets)\n",
    "\n",
    "# df.to_csv(\"data/data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se imaginan chicos agradeciendo premio cara or...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eclesiastes siempre promesa  {link}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edroj_ramirez qu√© sabor√≠o pj compartes gintoni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buenos dias todos feliz inicio semana  {link}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>epedom quintero gracias no as√≠ deja claro  aqu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Target\n",
       "0  se imaginan chicos agradeciendo premio cara or...       0\n",
       "1                eclesiastes siempre promesa  {link}       0\n",
       "2  edroj_ramirez qu√© sabor√≠o pj compartes gintoni...       0\n",
       "3      buenos dias todos feliz inicio semana  {link}       0\n",
       "4  epedom quintero gracias no as√≠ deja claro  aqu...       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data_cleaned.csv\").dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayloficial subire foto ustedes ig vengan villahermosa tomemos  ¬°ya vengaan'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Tweets[100300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################################\n",
    "\n",
    "logistic_pipe = Pipeline([(\"vect\", vectorizer), (\"cls\", LogisticRegression())]) # Logistic Regression\n",
    "\n",
    "logistic_params = {\"vect__max_df\": (0.5, 1), \"vect__min_df\": (10, 20, 50), \"cls__penalty\": [\"l1\",\"l2\"], \n",
    "\"cls__C\": [0.1, 1.0], \"cls__solver\" : [\"newton-cg\"]}\n",
    "\n",
    "log_reg = GridSearchCV(logistic_pipe, logistic_params, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "svc_pipe = Pipeline([(\"vect\", vectorizer), (\"cls\", LinearSVC())]) # Linear Support Vector Machine\n",
    "\n",
    "svc_params = {\"cls__C\": [0.001, 0.1, 1, 10, 100], \"cls__loss\": [\"hinge\", \"squared_hinge\"], \"cls__penalty\" : [\"l1\", \"l2\"]}\n",
    "\n",
    "svc = GridSearchCV(svc_pipe, svc_params, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "xgb = Pipeline([(\"vect\", vectorizer), (\"cls\", XGBClassifier())]) # XGB Classifier\n",
    "\n",
    "#####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Tweets\"], df[\"Target\"], test_size=0.20, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       ('cls', LogisticRegression())]),\n",
       "             param_grid={'cls__C': [0.1, 1.0], 'cls__penalty': ['l1', 'l2'],\n",
       "                         'cls__solver': ['newton-cg'], 'vect__max_df': (0.5, 1),\n",
       "                         'vect__min_df': (10, 20, 50)},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8110811194771985\n"
     ]
    }
   ],
   "source": [
    "log_reg_predictions = log_reg.predict(X_test)\n",
    "\n",
    "log_reg_accuraccy = accuracy_score(log_reg_predictions, y_test)\n",
    "\n",
    "print(log_reg_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/logistic_regression.h5\", 'wb') as save:\n",
    "    pickle.dump(log_reg, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(ngram_range=(1, 2))),\n",
       "                                       ('cls', LinearSVC())]),\n",
       "             param_grid={'cls__C': [0.001, 0.1, 1, 10, 100],\n",
       "                         'cls__loss': ['hinge', 'squared_hinge'],\n",
       "                         'cls__penalty': ['l1', 'l2']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8182412274470805\n"
     ]
    }
   ],
   "source": [
    "svc_predictions = svc.predict(X_test)\n",
    "\n",
    "svc_accuraccy = accuracy_score(svc_predictions, y_test)\n",
    "\n",
    "print(svc_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/LinearSVC.h5\", 'wb') as save:\n",
    "    pickle.dump(svc, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('cls',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "                               importance_type=None, interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_bin=256,\n",
       "                               max_cat_to_onehot=4, max_delta_step=0,\n",
       "                               max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "                               missing=nan, monotone_constraints='()',\n",
       "                               n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "                               predictor='auto', random_state=0, reg_alpha=0,\n",
       "                               reg_lambda=1, ...))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7914192356868873\n"
     ]
    }
   ],
   "source": [
    "xgb_predictions = xgb.predict(X_test)\n",
    "\n",
    "xgb_accuraccy = accuracy_score(xgb_predictions, y_test)\n",
    "\n",
    "print(xgb_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/XGBClassifier.h5\", 'wb') as save:\n",
    "    pickle.dump(xgb, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the strings into integers using Tokenizer\n",
    "\n",
    "max_vocab = 20000000\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of dataset vocab is: 128936\n"
     ]
    }
   ],
   "source": [
    "# Checking the word index and find out the vocabulary of the dataset\n",
    "\n",
    "wordidx = tokenizer.word_index\n",
    "\n",
    "print(f\"The size of dataset vocab is: {len(wordidx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequence: [50, 67, 72, 36, 16, 1]\n",
      "Test sequence: [675, 21]\n"
     ]
    }
   ],
   "source": [
    "# Converting train and test sentences into sequences\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "print(f\"Train sequence: {train_seq[0]}\")\n",
    "print(f\"Test sequence: {test_seq[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The len of train sequence is: 2093\n",
      "The len of test sequence is: 2093\n"
     ]
    }
   ],
   "source": [
    "# Padding the sentences to get equal length sequence because it's conventional to use same size sequences\n",
    "\n",
    "# Padding Train\n",
    "pad_train = pad_sequences(train_seq)\n",
    "\n",
    "print(f\"The len of train sequence is: {pad_train.shape[1]}\")\n",
    "\n",
    "\n",
    "# Padding test\n",
    "pad_test = pad_sequences(test_seq, maxlen=pad_train.shape[1])\n",
    "\n",
    "print(f\"The len of test sequence is: {pad_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "\n",
    "input_len = keras.layers.Input(shape=(pad_train.shape[1], ))\n",
    "\n",
    "x = keras.layers.Embedding(len(wordidx) + 1, 20)(input_len) # len(wordidx) + 1 because the indexing starts from 1, not from 0\n",
    "\n",
    "x = keras.layers.LSTM(25, return_sequences=True)(x)\n",
    "\n",
    "x = keras.layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "neural_network_model = keras.Model(input_len, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "neural_network_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "mcheckpoint = keras.callbacks.ModelCheckpoint(\"data/models/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4400/4400 [==============================] - 392s 88ms/step - loss: 0.4418 - accuracy: 0.8010 - val_loss: 0.4102 - val_accuracy: 0.8157\n",
      "Epoch 2/10\n",
      "4400/4400 [==============================] - 442s 100ms/step - loss: 0.3030 - accuracy: 0.8754 - val_loss: 0.4462 - val_accuracy: 0.8066\n",
      "Epoch 3/10\n",
      "4400/4400 [==============================] - 343s 78ms/step - loss: 0.2004 - accuracy: 0.9216 - val_loss: 0.5063 - val_accuracy: 0.7982\n",
      "Epoch 4/10\n",
      "4400/4400 [==============================] - 374s 85ms/step - loss: 0.1452 - accuracy: 0.9432 - val_loss: 0.5806 - val_accuracy: 0.7920\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "history = neural_network_model.fit(pad_train, y_train, validation_data=(pad_test, y_test), epochs=10, callbacks=[earlystop, mcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2093)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 2093, 20)          2578740   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 2093, 25)          4600      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 25)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,584,205\n",
      "Trainable params: 2,584,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_network_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 47s 43ms/step - loss: 0.4102 - accuracy: 0.8157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4101842939853668, 0.8157124519348145]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_model.evaluate(pad_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_model.save(\"data/models/neural_network.h5\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bdc6e612fb2213545a98f8a0a443d810ee75e809c56c9dac704b481d62b9359"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
