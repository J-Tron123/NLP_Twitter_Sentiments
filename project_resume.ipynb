{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis using NLTK libray\n",
    "\n",
    "In the present notebook we're going to use NLP (Natural Language Processing) techniques to analyze the sentiment of a text from Twitter, called Tweet, and build and train some of the most accurate machine learning models to the type of problem and generalizes better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we're going to import the following packages to attack the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, keras, pickle, warnings\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from models import CleanData\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv(\"data/tweets_pos_clean.csv\")\n",
    "negative = pd.read_csv(\"data/tweets_neg_clean.csv\") # Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive[\"Target\"] = [0 for i in positive[\"Tweets\"]] # Here we're sort out the data like this because is splitted in different datasets\n",
    "negative[\"Target\"] = [1 for i in negative[\"Tweets\"]] # but it's not part of the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives Tweets: 55056\n",
      "Negative Tweets: 120948\n"
     ]
    }
   ],
   "source": [
    "print(\"Positives Tweets:\", len(positive))\n",
    "print(\"Negative Tweets:\", len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Se imaginan a los chicos agradeciendo por el p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eclesiastes4:9-12 ‚ô° Siempre, promesa :)  https...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@pedroj_ramirez Qu√© sabor√≠o, PJ. ya no compart...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Buenos dias para todos. Feliz inicio de semana...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@pepedom @bquintero Gracias! No es as√≠, deja c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175999</th>\n",
       "      <td>Pero... Dime que no te perder√© del todo :( ‚ù§üíõüíö</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176000</th>\n",
       "      <td>Yo creo que a Colocolo le hac√≠a falta un parti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176001</th>\n",
       "      <td>@seru15 son para ni√±o :( quisiera quedarmelos.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176002</th>\n",
       "      <td>Diganle al sonidero que ya le baje a su desmad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176003</th>\n",
       "      <td>Buenos d√≠as, gente que se le da√±a el celular :(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176004 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweets  Target\n",
       "0       Se imaginan a los chicos agradeciendo por el p...       0\n",
       "1       Eclesiastes4:9-12 ‚ô° Siempre, promesa :)  https...       0\n",
       "2       @pedroj_ramirez Qu√© sabor√≠o, PJ. ya no compart...       0\n",
       "3       Buenos dias para todos. Feliz inicio de semana...       0\n",
       "4       @pepedom @bquintero Gracias! No es as√≠, deja c...       0\n",
       "...                                                   ...     ...\n",
       "175999     Pero... Dime que no te perder√© del todo :( ‚ù§üíõüíö       1\n",
       "176000  Yo creo que a Colocolo le hac√≠a falta un parti...       1\n",
       "176001     @seru15 son para ni√±o :( quisiera quedarmelos.       1\n",
       "176002  Diganle al sonidero que ya le baje a su desmad...       1\n",
       "176003    Buenos d√≠as, gente que se le da√±a el celular :(       1\n",
       "\n",
       "[176004 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = positive.merge(negative, how=\"outer\") # Merge the to sorted dataframes in one\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the cleaning functions to our raw data to get a processed data for get better accuracy to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_links) \n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().clean_emojis)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_stopwords)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().signs_tweets)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_doubles)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().clean_laughs)\n",
    "# df[\"Tweets\"] = df[\"Tweets\"].apply(CleanData().remove_mentions_hashtags_retweets)\n",
    "\n",
    "# df.dropna(axis=0).to_csv(\"data/data_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351950\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>se imaginan chicos agradeciendo premio cara or...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eclesiastes siempre promesa  {link}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edroj_ramirez qu√© sabor√≠o pj compartes gintoni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buenos dias todos feliz inicio semana  {link}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>epedom quintero gracias no as√≠ deja claro  aqu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Target\n",
       "0  se imaginan chicos agradeciendo premio cara or...       0\n",
       "1                eclesiastes siempre promesa  {link}       0\n",
       "2  edroj_ramirez qu√© sabor√≠o pj compartes gintoni...       0\n",
       "3      buenos dias todos feliz inicio semana  {link}       0\n",
       "4  epedom quintero gracias no as√≠ deja claro  aqu...       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data_cleaned.csv\").dropna()\n",
    "print(df.size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayloficial subire foto ustedes ig vengan villahermosa tomemos  ¬°ya vengaan'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Tweets[100300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the vocabulary of the processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building pipelines that first vectorize the data and then insert the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################################\n",
    "\n",
    "logistic_pipe = Pipeline([(\"vect\", vectorizer), (\"cls\", LogisticRegression())]) # Logistic Regression\n",
    "\n",
    "logistic_params = {\"vect__max_df\": (0.5, 1), \"vect__min_df\": (10, 20, 50), \"cls__penalty\": [\"l1\",\"l2\"], \n",
    "\"cls__C\": [0.1, 1.0], \"cls__solver\" : [\"newton-cg\"]}\n",
    "\n",
    "log_reg = GridSearchCV(logistic_pipe, logistic_params, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "svc_pipe = Pipeline([(\"vect\", vectorizer), (\"cls\", LinearSVC())]) # Linear Support Vector Machine\n",
    "\n",
    "svc_params = {\"cls__C\": [0.001, 0.1, 1, 10, 100], \"cls__loss\": [\"hinge\", \"squared_hinge\"], \"cls__penalty\" : [\"l1\", \"l2\"]}\n",
    "\n",
    "svc = GridSearchCV(svc_pipe, svc_params, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "#####################################################################################################################################\n",
    "\n",
    "xgb = Pipeline([(\"vect\", vectorizer), (\"cls\", XGBClassifier())]) # XGB Classifier\n",
    "\n",
    "#####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"Tweets\"], df[\"Target\"], test_size=0.20, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/models/logistic_regression.h5\", \"wb\") as save:\n",
    "#     pickle.dump(log_reg, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/logistic_regression.h5\", \"rb\") as load:\n",
    "    log_reg = pickle.load(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8110811194771985\n"
     ]
    }
   ],
   "source": [
    "log_reg_predictions = log_reg.predict(X_test)\n",
    "\n",
    "log_reg_accuraccy = accuracy_score(log_reg_predictions, y_test)\n",
    "\n",
    "print(log_reg_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/models/LinearSVC.h5\", \"wb\") as save:\n",
    "#     pickle.dump(svc, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/LinearSVC.h5\", \"rb\") as load:\n",
    "    svc = pickle.load(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8182412274470805\n"
     ]
    }
   ],
   "source": [
    "svc_predictions = svc.predict(X_test)\n",
    "\n",
    "svc_accuraccy = accuracy_score(svc_predictions, y_test)\n",
    "\n",
    "print(svc_accuraccy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/models/XGBClassifier.h5\", \"wb\") as save:\n",
    "#     pickle.dump(xgb, save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/models/XGBClassifier.h5\", \"rb\") as load:\n",
    "    xgb = pickle.load(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7914192356868873\n"
     ]
    }
   ],
   "source": [
    "xgb_predictions = xgb.predict(X_test)\n",
    "\n",
    "xgb_accuraccy = accuracy_score(xgb_predictions, y_test)\n",
    "\n",
    "print(xgb_accuraccy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the strings into integers using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 20000000\n",
    "tokenizer = Tokenizer(num_words=max_vocab)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the word index and find out the vocabulary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of dataset vocab is: 128936\n"
     ]
    }
   ],
   "source": [
    "wordidx = tokenizer.word_index\n",
    "\n",
    "print(f\"The size of dataset vocab is: {len(wordidx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting train and test sentences into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequence: [50, 67, 72, 36, 16, 1]\n",
      "Test sequence: [675, 21]\n"
     ]
    }
   ],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "print(f\"Train sequence: {train_seq[0]}\")\n",
    "print(f\"Test sequence: {test_seq[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding the sentences to get equal length sequence because it's conventional to use same size sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The len of train sequence is: 2093\n",
      "The len of test sequence is: 2093\n"
     ]
    }
   ],
   "source": [
    "# Padding Train\n",
    "pad_train = pad_sequences(train_seq)\n",
    "\n",
    "print(f\"The len of train sequence is: {pad_train.shape[1]}\")\n",
    "\n",
    "\n",
    "# Padding test\n",
    "pad_test = pad_sequences(test_seq, maxlen=pad_train.shape[1])\n",
    "\n",
    "print(f\"The len of test sequence is: {pad_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_len = keras.layers.Input(shape=(pad_train.shape[1], ))\n",
    "\n",
    "# x = keras.layers.Embedding(len(wordidx) + 1, 20)(input_len) # len(wordidx) + 1 because the indexing starts from 1, not from 0\n",
    "\n",
    "# x = keras.layers.LSTM(25, return_sequences=True)(x)\n",
    "\n",
    "# x = keras.layers.GlobalMaxPool1D()(x)\n",
    "\n",
    "# x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "# x = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# neural_network_model = keras.Model(input_len, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_network_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# earlystop = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "# mcheckpoint = keras.callbacks.ModelCheckpoint(\"data/models/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = neural_network_model.fit(pad_train, y_train, validation_data=(pad_test, y_test), epochs=10, callbacks=[earlystop, mcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_network_model.save(\"data/models/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_model = keras.models.load_model(\"data/models/neural_network.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2093)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 2093, 20)          2578740   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 2093, 25)          4600      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 25)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                832       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,584,205\n",
      "Trainable params: 2,584,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural_network_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 86s 78ms/step - loss: 0.4159 - accuracy: 0.8138\n"
     ]
    }
   ],
   "source": [
    "neural_network_accuracy = neural_network_model.evaluate(pad_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Support Vector Machine</td>\n",
       "      <td>0.818241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>0.813837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.791419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Accuracy\n",
       "1  Linear Support Vector Machine  0.818241\n",
       "3                 Neural Network  0.813837\n",
       "0              Linear Regression  0.811081\n",
       "2                        XGBoost  0.791419"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Model\" : [\"Linear Regression\", \"Linear Support Vector Machine\", \"XGBoost\", \"Neural Network\"],\n",
    "                        \"Accuracy\" : [log_reg_accuraccy, svc_accuraccy, xgb_accuraccy, neural_network_accuracy[1]]})\n",
    "\n",
    "results.sort_values(by=\"Accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per thousandths, the Linear Support Vector Machine is the model that best generalizes with an accuracy of 0.818241, followed by the Neural Network and Linear Regression an accuracy of 0.813837 and 0.811081, respectively. We can see that the difference between metrics is very narrow, so even if the first mentioned generalizes better, it could depend on the data that is inserted than some of the better or worse results, although they will always maintain a very narrow difference between them."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0bdc6e612fb2213545a98f8a0a443d810ee75e809c56c9dac704b481d62b9359"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
